{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Brief introduction to Neural Networks:\nArtificial neural networks (ANNs), usually called neural networks (NNs) or, more simply yet, neural nets,are  computing systems inspired by the biological neural networks that constitute animal brains.\n\nAn ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. \n\nThese neurons are nothing but mathematical functions which, when given some input, generate an output. The output of neurons depends on the input and the parameters of the neurons. We can update these parameters to get a desired value out of the network.\n\nEach of these neurons are defined using sigmoid function. A sigmoid function gives an output between zero to one for every input it gets. These sigmoid units are connected to each other to form a neural network.\n\nBy connection here we mean that the output of one layer of sigmoid units is given as input to each sigmoid unit of the next layer. In this way our neural network produces an output for any given input. The process continues until we have reached the final layer. The final layer generates its output.\n\nThis process of a neural network generating an output for a given input is Forward Propagation. Output of final layer is also called the prediction of the neural network. \n\nRight after the final layer generates its output, we calculate the cost function. The cost function computes how far our neural network is from making its desired predictions. The value of the cost function shows the difference between the predicted value and the truth value.\n\nOur objective here is to minimize the value of the cost function. The process of minimization of the cost function requires an algorithm which can update the values of the parameters in the network in such a way that the cost function achieves its minimum value.\n\nAlgorithms such as gradient descent and stochastic gradient descent are used to update the parameters of the neural network. These algorithms update the values of weights and biases of each layer in the network depending on how it will affect the minimization of cost function. The effect on the minimization of the cost function with respect to each of the weights and biases of each of the input neurons in the network is computed by backpropagation.\n\nNeural networks are essentially self-optimizing functions that map inputs to the correct outputs. We can then place a new input into the function, where it will predict an output based on the function it created with the training data.\n\nOUR PROBLEM :-\nHere,we will solve a simple problem. Suppose we have some information about obesity, smoking habits, and exercise habits of six people. We also know if these people have high cholesterol. Our dataset looks like this:\nOBESITY    PHYSICAL ACTIVITY/EXERCISES     SMOKING      CHOLESTEROL\n  0                    1                      0             0\n  0                    1                      1             0     \n  0                    0                      0             0\n  1                    0                      0             1\n  1                    1                      1             1\n  1                    0                      1             1\n\nThis is a type of supervised learning problem where we are given inputs and corresponding correct outputs and our task is to find the mapping between the inputs and the outputs.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np ","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:13:58.228987Z","iopub.execute_input":"2022-07-20T16:13:58.229615Z","iopub.status.idle":"2022-07-20T16:13:58.259699Z","shell.execute_reply.started":"2022-07-20T16:13:58.229486Z","shell.execute_reply":"2022-07-20T16:13:58.258497Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"input_matrix = np.array([[0, 1, 0],[0, 1, 1],[0, 0, 0],[1, 0, 0],[1, 1, 1],[1, 0, 1]])            \n                   \n\noutput_matrix = np.array([[0], [0], [0], [1], [1], [1]])","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:13:58.387258Z","iopub.execute_input":"2022-07-20T16:13:58.388168Z","iopub.status.idle":"2022-07-20T16:13:58.395678Z","shell.execute_reply.started":"2022-07-20T16:13:58.388107Z","shell.execute_reply":"2022-07-20T16:13:58.394696Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"As mentioned earlier, neural networks need data to learn from. We will create our input data matrix and the corresponding outputs matrix with Numpy’s .array() function. ","metadata":{}},{"cell_type":"code","source":"class NeuralNetwork:\n\n    #1\n    def __init__(self, input_matrix, output_matrix):\n        self.inputs  = input_matrix\n        self.outputs = output_matrix\n        \n        \n        np.random.seed(5)\n        self.weights = np.random.rand(3,1)\n\n    #2    \n    def sigmoid(self,x):\n        return 1 / (1 + np.exp(-x))\n    \n    def sigmoid_prime(self, x):\n        return x * (1 - x)\n    \n    #3\n    def forward_propagation(self):\n        self.hidden = self.sigmoid(np.dot(self.inputs, self.weights))\n        \n        \n    #4   \n    def back_propagation(self):\n        self.error  = self.outputs - self.hidden\n        delta = self.error * self.sigmoid_prime(self.hidden)\n        self.weights += np.dot(self.inputs.T, delta)\n        \n    #5    \n    def train_neuralnetwork(self, iterations=50000):\n        for iteration in range(iterations):\n            \n            self.forward_propagation()\n            \n            self.back_propagation()\n            \n    #6        \n    def predict(self, new_input):\n        prediction = self.sigmoid(np.dot(new_input, self.weights))\n        return prediction       \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:13:58.490392Z","iopub.execute_input":"2022-07-20T16:13:58.491461Z","iopub.status.idle":"2022-07-20T16:13:58.504627Z","shell.execute_reply.started":"2022-07-20T16:13:58.491392Z","shell.execute_reply":"2022-07-20T16:13:58.503391Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#1: We start by creating a class called “NeuralNetwork”. We initialize the class by defining the __init__ function. It takes the input_matrix and the output_matrix as the parameter. In the script above we used the random.seed function so that we can get the same random values whenever the script is executed.In the next step, we initialize our weights with normally distributed random numbers. Since we have three features in the input, we have a vector of three weights.","metadata":{}},{"cell_type":"code","source":"#2","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:13:58.580772Z","iopub.execute_input":"2022-07-20T16:13:58.581769Z","iopub.status.idle":"2022-07-20T16:13:58.587503Z","shell.execute_reply.started":"2022-07-20T16:13:58.581700Z","shell.execute_reply":"2022-07-20T16:13:58.586150Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"This neural network will be using the sigmoid function as the activation function.\nLet us first define what a sigmoid function is:An Activation Function decides whether a neuron should be activated or not. This means that it will decide whether the neuron’s input to the network is important or not in the process of prediction using simpler mathematical operations. \n\nThe role of the Activation Function is to derive output from a set of input values fed to a node (or a layer).\nDepending on the nature and intensity of these input signals, the brain processes them and decides whether the neuron should be activated (“fired”) or not. \n\nIn deep learning, this is also the role of the Activation Function—that’s why it’s often referred to as a Transfer Function in Artificial Neural Network.  \n\nThe primary role of the Activation Function is to transform the summed weighted input from the node into an output value to be fed to the next hidden layer or as output. \n\n\nWhy do Neural Networks need it?\nThe purpose of an activation function is to add non-linearity to the neural network.\n\nHow Activation Function works in Neural Networks\nActivation functions introduce an additional step at each layer during the forward propagation, but its computation is worth it. \n\nHere, we use the sigmoid function as our activation function.\nThe sigmoid function is a popular nonlinear activation function that has a range of (0–1). The inputs to this function will always be squished down to fit in-between the sigmoid function’s two horizontal asymptotes at y=0 and y=1. The sigmoid function has some well-known issues that restrict its usage. When we look at the graph below of the sigmoidal curve, we notice that as we reach the two ends of the curve, the derivatives of those points become very small. When these small derivatives are multiplied during backpropagation, they become smaller and smaller until becoming useless. Due to the derivatives, or gradients, getting smaller and smaller, the weights in the neural network will not be updated very much, if at all. This will lead the neural network to become stuck, with the situation becoming worse and worse for every additional training iteration.","metadata":{}},{"cell_type":"code","source":"#3","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:13:58.676604Z","iopub.execute_input":"2022-07-20T16:13:58.677549Z","iopub.status.idle":"2022-07-20T16:13:58.683277Z","shell.execute_reply.started":"2022-07-20T16:13:58.677483Z","shell.execute_reply":"2022-07-20T16:13:58.681884Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"During our neural network’s training process, the input data will be fed forward through the network’s weights and functions. The result of this feed-forward function will be the output of the hidden layer.  The forward propagation function can be written like this, where xᵢ and wᵢ are individual features and weights in the matrices:\ny cap = 1/(1 + e^-z) where z: sigma (xi * wi) + b\nHere b is the bias term. For simplicity reasons we will consider b = 0.\nEach feature in the input data will have its own weight for its connection to the hidden layer. We will start by taking the sum of every feature multiplied by its corresponding weight. We can take the results and feed it through the sigmoid function to get a value(probability) between (0–1).\n\nThe above process will result in the hidden layer’s prediction. Each row in the sigma(xw) matrix will be entered into the sigmoid function. The colours represent the individual processes for each row in the sigma(xw) matrix. \nNote: this calculation only represents one training iteration, so the resulting y cap matrix will not be very accurate. By computing the hidden layer this way, then using backpropagation for many iterations, the result will be much more accurate.\n","metadata":{}},{"cell_type":"code","source":"#4","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:13:58.770446Z","iopub.execute_input":"2022-07-20T16:13:58.770918Z","iopub.status.idle":"2022-07-20T16:13:58.776434Z","shell.execute_reply.started":"2022-07-20T16:13:58.770878Z","shell.execute_reply":"2022-07-20T16:13:58.775016Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Backpropagation will go back through the layer(s) of the neural network, determine which weights contributed to the output and the error, then change the weights based on the gradient of the hidden layers output. The whole process can be written like this, where y is the correct output and y cap is the hidden layers prediction:\n\nwi + XT.(y - y cap).(S * (1 - S)) where S is 1/(1 + e^-z)\n\nWe can now multiply the error and the derivative of the hidden layer’s prediction. We know that the derivative of the sigmoid function is S(x)(1 — S(x)). Therefore, the derivative for each of the hidden layer’s predictions would be y cap*(1 - y cap)\nThis step will result with the update that will be added to the weights. We can get this update by multiplying our “error weighted derivative” from the above step and the inputs.This step will result with the update that will be added to the weights. We can get this update by multiplying our “error weighted derivative” from the above step and the inputs. Once we have the updated matrix, we can add it to our weights matrix to officially change the weights to become stronger.\n\n\n","metadata":{}},{"cell_type":"code","source":"#5\n      ","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:13:58.866864Z","iopub.execute_input":"2022-07-20T16:13:58.867390Z","iopub.status.idle":"2022-07-20T16:13:58.873389Z","shell.execute_reply.started":"2022-07-20T16:13:58.867336Z","shell.execute_reply":"2022-07-20T16:13:58.871528Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The time has come to train the neural network. During the training process, the neural network will “learn” which features in the input data correlate with its output, and it will learn to make accurate predictions. To train our neural network, we create a train_neuralnetwork function with the number of iterations to 50,000. This means the neural network will repeat the weight-updating process 50,000 times. Within the train function, we will call our forward_propagation() function, then the back_propagation() function. ","metadata":{}},{"cell_type":"code","source":"#6                       \n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:13:58.960490Z","iopub.execute_input":"2022-07-20T16:13:58.961357Z","iopub.status.idle":"2022-07-20T16:13:58.967373Z","shell.execute_reply.started":"2022-07-20T16:13:58.961309Z","shell.execute_reply":"2022-07-20T16:13:58.966032Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The prediction function will look similar to the hidden layer. The forward propagation function essentially makes a prediction as well, then backpropagation checks for the error and updates the weights. Our predict function will use the same method as the feedforward function: multiply the input matrix and the weights matrix, then feed the results through the sigmoid function to return a value between 0-1. ","metadata":{}},{"cell_type":"code","source":"  \nNN = NeuralNetwork(input_matrix, output_matrix)\nNN.train_neuralnetwork()","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:13:59.037120Z","iopub.execute_input":"2022-07-20T16:13:59.038014Z","iopub.status.idle":"2022-07-20T16:14:00.485259Z","shell.execute_reply.started":"2022-07-20T16:13:59.037951Z","shell.execute_reply":"2022-07-20T16:14:00.483230Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We will create our NN object from the NeuralNetwork class and pass in the input matrix and the output matrix. We can then call the .train_neuralnetwork() function on our object.","metadata":{}},{"cell_type":"code","source":"                                   \nfirst = np.array([[1, 1, 0]])\nsecond = np.array([[0, 1, 1]])\n\n                                   \nprint(NN.predict(first), ' ANSWER: ', first[0][0])\nprint(NN.predict(second), ' ANSWER: ', second[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-07-20T16:14:00.517089Z","iopub.execute_input":"2022-07-20T16:14:00.517697Z","iopub.status.idle":"2022-07-20T16:14:00.539367Z","shell.execute_reply.started":"2022-07-20T16:14:00.517647Z","shell.execute_reply":"2022-07-20T16:14:00.537898Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Now we can create the two new examples that we want our neural network to make predictions for. We will call these “first” and “second”. We can then call the .predict() function and pass through the arrays. We can guess from our original table that the first number in the input determines the output. The first example, “first”, has a 1 in the first column, and therefore the output should be a 1. The second example has a 0 in the first column, and so the output should be a 0.","metadata":{}}]}